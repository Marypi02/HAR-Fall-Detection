_target_: net.rnn.LSTMBlock 
input_size: 128
hidden_size: 64 # 128
num_layers: 1 # 2
dropout: 0.3 # 0.1 # % di neuroni disattivati causalmente per ogni batch per evitare la co-dipendenza e forzare la ridondanza nell'apprendimento
bidirectional: True # since we need only past context
